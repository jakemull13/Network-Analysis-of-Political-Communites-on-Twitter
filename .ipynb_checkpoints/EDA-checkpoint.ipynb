{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual PreProcessing (Lower Case All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all lowercase\n",
    "def lower_df_text(df, text_columns):\n",
    "    for column in text_columns:\n",
    "        df[column] = df[column].apply(lambda x: x.lower())\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(text, figname='wordcloud', rgb=(255,255,255)):\n",
    "    font_path = \"/Library/Fonts/DIN Condensed Bold.ttf\"\n",
    "    wc = WordCloud(background_color=\"white\",max_words=25,\n",
    "                   collocations=False, font_path=font_path, scale=5, color_func=lambda *args, **kwargs: rgb)\n",
    "    wc.generate(text)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14,18))\n",
    "    plt.imshow(wc)\n",
    "    plt.savefig('data/wordclouds/{}'.format(figname), dpi=240)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Network Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functionize\n",
    "\n",
    "def generate_graph(edge_filepath, node_filepath, title='gephi_graph'):\n",
    "    edges = pd.read_csv(edge_filepath, header=None)\n",
    "    nodes = pd.read_csv(node_filepath, header=None)\n",
    "    edges.columns = ['Source', 'Target']\n",
    "    nodes.columns = ['Node', 'Alias']\n",
    "    nodes = nodes.astype('str')\n",
    "    edges = edges.astype('str')\n",
    "    edge_tuples = []\n",
    "    for source, target in zip(edges['Source'], edges['Target']):\n",
    "        edge_tuples.append((source, target))\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(nodes['Node'])\n",
    "    G.add_edges_from(edge_tuples)\n",
    "    print(nx.info(G))\n",
    "    nx.write_gexf(G, edge_filepath.append('{}'.format(title)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing Pipeline\n",
    "from https://www.kaggle.com/balatmak/text-preprocessing-steps-and-universal-pipeline#Reusable-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import string\n",
    "import spacy \n",
    "import en_core_web_sm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from normalise import normalise\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 variety=\"BrE\",\n",
    "                 user_abbrevs={},\n",
    "                 n_jobs=1, custom_stop_words=[]):\n",
    "        \"\"\"\n",
    "        Text preprocessing transformer includes steps:\n",
    "            1. Text normalization\n",
    "            2. Punctuation removal\n",
    "            3. Stop words removal\n",
    "            4. Lemmatization\n",
    "        \n",
    "        variety - format of date (AmE - american type, BrE - british format) \n",
    "        user_abbrevs - dict of user abbreviations mappings (from normalise package)\n",
    "        n_jobs - parallel jobs to run\n",
    "        \"\"\"\n",
    "        self.variety = variety\n",
    "        self.user_abbrevs = user_abbrevs\n",
    "        self.n_jobs = n_jobs\n",
    "        self.custom_stop_words=custom_stop_words\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        partitions = 1\n",
    "        cores = mp.cpu_count()\n",
    "        if self.n_jobs <= -1:\n",
    "            partitions = cores\n",
    "        elif self.n_jobs <= 0:\n",
    "            return X_copy.apply(self._preprocess_text)\n",
    "        else:\n",
    "            partitions = min(self.n_jobs, cores)\n",
    "\n",
    "        data_split = np.array_split(X_copy, partitions)\n",
    "        pool = mp.Pool(cores)\n",
    "        data = pd.concat(pool.map(self._preprocess_part, data_split))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _preprocess_part(self, part):\n",
    "        return part.apply(self._preprocess_text)\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        normalized_text = self._normalize(text)\n",
    "        doc = nlp(normalized_text)\n",
    "        removed_punct = self._remove_punct(doc)\n",
    "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
    "        return self._lemmatize(removed_stop_words)\n",
    "\n",
    "    def _normalize(self, text):\n",
    "        # some issues in normalise package\n",
    "        try:\n",
    "            return ' '.join(normalise(text, variety=self.variety, user_abbrevs=self.user_abbrevs, verbose=False))\n",
    "        except:\n",
    "            return text\n",
    "\n",
    "    def _remove_punct(self, doc):\n",
    "        return [t for t in doc if t.text not in string.punctuation]\n",
    "\n",
    "    def _remove_stop_words(self, doc):\n",
    "        return [t for t in doc if t.is_stop != True and t.text not in self.custom_stop_words  and 'https' not in t.text]\n",
    "\n",
    "    def _lemmatize(self, doc):\n",
    "        return ' '.join([t.lemma_ for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Democratic Convention Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reweet_id</th>\n",
       "      <th>retweet_screen_name</th>\n",
       "      <th>user_favourites_count</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>user_listed_count</th>\n",
       "      <th>user_statuses_count</th>\n",
       "      <th>user_time_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>134687.000000</td>\n",
       "      <td>1.346870e+05</td>\n",
       "      <td>7.540000e+03</td>\n",
       "      <td>1.724400e+04</td>\n",
       "      <td>134687.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.346870e+05</td>\n",
       "      <td>1.346870e+05</td>\n",
       "      <td>134687.000000</td>\n",
       "      <td>134687.000000</td>\n",
       "      <td>1.346870e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.345653</td>\n",
       "      <td>7.582166e+17</td>\n",
       "      <td>7.576638e+17</td>\n",
       "      <td>1.624101e+16</td>\n",
       "      <td>1.681061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.216183e+04</td>\n",
       "      <td>9.140628e+03</td>\n",
       "      <td>2767.032609</td>\n",
       "      <td>133.059946</td>\n",
       "      <td>4.797842e+04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>93.861568</td>\n",
       "      <td>5.226539e+14</td>\n",
       "      <td>1.226524e+16</td>\n",
       "      <td>1.073660e+17</td>\n",
       "      <td>60.445901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.600100e+04</td>\n",
       "      <td>9.132616e+04</td>\n",
       "      <td>7243.250845</td>\n",
       "      <td>484.443211</td>\n",
       "      <td>9.282556e+04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.564538e+17</td>\n",
       "      <td>4.238422e+16</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.577726e+17</td>\n",
       "      <td>7.576961e+17</td>\n",
       "      <td>1.651195e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.686000e+03</td>\n",
       "      <td>3.920000e+02</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.261000e+03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.581443e+17</td>\n",
       "      <td>7.580650e+17</td>\n",
       "      <td>5.524592e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.019400e+04</td>\n",
       "      <td>1.110000e+03</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>1.779900e+04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.586317e+17</td>\n",
       "      <td>7.584846e+17</td>\n",
       "      <td>5.266899e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.172300e+04</td>\n",
       "      <td>3.352000e+03</td>\n",
       "      <td>2645.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>4.917500e+04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>30492.000000</td>\n",
       "      <td>7.594183e+17</td>\n",
       "      <td>7.594173e+17</td>\n",
       "      <td>7.591036e+17</td>\n",
       "      <td>20768.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.053126e+06</td>\n",
       "      <td>1.372433e+07</td>\n",
       "      <td>317437.000000</td>\n",
       "      <td>30229.000000</td>\n",
       "      <td>2.230300e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       favorite_count            id  in_reply_to_status_id  \\\n",
       "count   134687.000000  1.346870e+05           7.540000e+03   \n",
       "mean         3.345653  7.582166e+17           7.576638e+17   \n",
       "std         93.861568  5.226539e+14           1.226524e+16   \n",
       "min          0.000000  7.564538e+17           4.238422e+16   \n",
       "25%          0.000000  7.577726e+17           7.576961e+17   \n",
       "50%          0.000000  7.581443e+17           7.580650e+17   \n",
       "75%          1.000000  7.586317e+17           7.584846e+17   \n",
       "max      30492.000000  7.594183e+17           7.594173e+17   \n",
       "\n",
       "       in_reply_to_user_id  retweet_count  reweet_id  retweet_screen_name  \\\n",
       "count         1.724400e+04  134687.000000        0.0                  0.0   \n",
       "mean          1.624101e+16       1.681061        NaN                  NaN   \n",
       "std           1.073660e+17      60.445901        NaN                  NaN   \n",
       "min           1.200000e+01       0.000000        NaN                  NaN   \n",
       "25%           1.651195e+07       0.000000        NaN                  NaN   \n",
       "50%           5.524592e+07       0.000000        NaN                  NaN   \n",
       "75%           5.266899e+08       0.000000        NaN                  NaN   \n",
       "max           7.591036e+17   20768.000000        NaN                  NaN   \n",
       "\n",
       "       user_favourites_count  user_followers_count  user_friends_count  \\\n",
       "count           1.346870e+05          1.346870e+05       134687.000000   \n",
       "mean            3.216183e+04          9.140628e+03         2767.032609   \n",
       "std             6.600100e+04          9.132616e+04         7243.250845   \n",
       "min             0.000000e+00          0.000000e+00            0.000000   \n",
       "25%             2.686000e+03          3.920000e+02          520.000000   \n",
       "50%             1.019400e+04          1.110000e+03         1166.000000   \n",
       "75%             3.172300e+04          3.352000e+03         2645.000000   \n",
       "max             1.053126e+06          1.372433e+07       317437.000000   \n",
       "\n",
       "       user_listed_count  user_statuses_count  user_time_zone  \n",
       "count      134687.000000         1.346870e+05             0.0  \n",
       "mean          133.059946         4.797842e+04             NaN  \n",
       "std           484.443211         9.282556e+04             NaN  \n",
       "min             0.000000         1.000000e+00             NaN  \n",
       "25%            11.000000         6.261000e+03             NaN  \n",
       "50%            34.000000         1.779900e+04             NaN  \n",
       "75%            97.000000         4.917500e+04             NaN  \n",
       "max         30229.000000         2.230300e+06             NaN  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Index(['coordinates', 'created_at', 'hashtags', 'media', 'urls',\n",
       "       'favorite_count', 'id', 'in_reply_to_screen_name',\n",
       "       'in_reply_to_status_id', 'in_reply_to_user_id', 'lang', 'place',\n",
       "       'possibly_sensitive', 'retweet_count', 'reweet_id',\n",
       "       'retweet_screen_name', 'source', 'text', 'tweet_url', 'user_created_at',\n",
       "       'user_screen_name', 'user_default_profile_image', 'user_description',\n",
       "       'user_favourites_count', 'user_followers_count', 'user_friends_count',\n",
       "       'user_listed_count', 'user_location', 'user_name', 'user_screen_name.1',\n",
       "       'user_statuses_count', 'user_time_zone', 'user_urls', 'user_verified'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dconvention_df = pd.read_csv('data/democrat/convention-tweet-ids.csv')\n",
    "dconvention_df.describe()\n",
    "dconvention_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Republican Convention Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Democratic Candidates Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dcandidates_df = pd.read_csv('data/democrat/dcandidate-tweet-ids.csv')\n",
    "#lowercase\n",
    "lower_df_text(dcandidates_df, ['text'])\n",
    "#text process\n",
    "dcandidate_text = TextPreprocessor(n_jobs=-1, custom_stop_words=['de', 'hashtag', 'year', 'today', 'co']  ).fit_transform(dcandidates_df.text)\n",
    "#replace text column with processed text\n",
    "dcandidates_df['text'] = dcandidate_text\n",
    "#re-add the mentions and hashtags text\n",
    "dcandidates_df['hashtags'].fillna(value='', inplace=True)\n",
    "dcandidates_df['in_reply_to_screen_name'].fillna(value='', inplace=True)\n",
    "dcandidates_df['text'] = dcandidates_df['text'] + dcandidates_df['hashtags'] + dcandidates_df['in_reply_to_screen_name']\n",
    "#wordcloud\n",
    "plot_wordcloud(' '.join(rcandidate_text), figname='democratic_candidates', rgb=(0,0,255))\n",
    "#save output\n",
    "dcandidates_df.to_csv('/Users/jacobmullins/data-science-immersive/capstone_2/data/democrat/dcandidates_df.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#save text output only (for AWS)\n",
    "dcandidates_df['text'].to_csv('/Users/jacobmullins/data-science-immersive/capstone_2/data/democrat/dcandidates_text.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Republican Candidates Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rcandidates_df = pd.read_csv('data/republican/rcandidate-tweet-ids.csv')\n",
    "#lowercase\n",
    "lower_df_text(rcandidates_df, ['text'])\n",
    "#text process\n",
    "rcandidate_text = TextPreprocessor(n_jobs=-1, custom_stop_words=['de', 'hashtag', 'year', 'today', 'co', 'bit', 'ly', 'twitpic', 'bit.ly']  ).fit_transform(rcandidates_df.text)\n",
    "#replace text column with processed text\n",
    "rcandidates_df['text'] = rcandidate_text\n",
    "#re-add the mentions and hashtags text\n",
    "rcandidates_df['hashtags'].fillna(value='', inplace=True)\n",
    "rcandidates_df['in_reply_to_screen_name'].fillna(value='', inplace=True)\n",
    "rcandidates_df['text'] = rcandidates_df['text'] + rcandidates_df['hashtags'] + rcandidates_df['in_reply_to_screen_name']\n",
    "#wordcloud\n",
    "plot_wordcloud(' '.join(rcandidate_text), figname='republican_candidates', rgb=(255,0,0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#save output\n",
    "rcandidates_df.to_csv('/Users/jacobmullins/data-science-immersive/capstone_2/data/republican/rcandidates_df.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#save text output only (for AWS)\n",
    "rcandidates_df['text'].to_csv('/Users/jacobmullins/data-science-immersive/capstone_2/data/republican/rcandidates_text.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
